# -*- coding: utf-8 -*-
"""SSD_PyTorch

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w4Svq_UdSomkR6ugSd_02K17K5_Yczb2

## Setup
"""
from __future__ import annotations
import gymnasium as gym
import math
import random
import matplotlib
import matplotlib.pyplot as plt
from collections import namedtuple, deque
from itertools import count
import numpy as np

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

from tqdm import tqdm
from collections import defaultdict

from gymnasium.spaces import Box, Dict, Discrete, MultiDiscrete, Tuple
import seaborn as sns
from matplotlib.colors import ListedColormap
import matplotlib.pyplot as plt
from matplotlib import colors
from ray.rllib.env import MultiAgentEnv

from agents.cleanup_agent import CleanupAgent, GreedyCleanUpAgent

# set up matplotlib
is_ipython = 'inline' in matplotlib.get_backend()
if is_ipython:
    from IPython import display

plt.ion()

# if GPU is to be used
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"cuda available: {torch.cuda.is_available()}")
np.set_printoptions(threshold=np.inf)


num_agents = 10
reward_multiplier = 10

# for printing options
pp = False
verbose = False
verbose_episode = 1999


"""Network Definitions"""


def preprocess_inputs(env_states):
    agent_maps = None
    a_keys = sorted(env_states.keys())  # key must be sorted
    for k in a_keys:
        if agent_maps is None:
            agent_maps = env_states[k]
        else:
            agent_maps = np.concatenate((agent_maps, env_states[k]), axis=0)
    # Stack all maps along the channel dimension
    return agent_maps


class UNetwork(nn.Module):
    def __init__(self, num_agents):
        super(UNetwork, self).__init__()
        self.pref_embedding = 1
        self.coord1 = nn.Linear(4, 64)
        self.coord2 = nn.Linear(64, 32)
        self.coord3 = nn.Linear(32, self.pref_embedding)
        self.leaky_relu = nn.LeakyReLU()
        torch.nn.init.xavier_uniform_(self.coord1.weight)
        torch.nn.init.xavier_uniform_(self.coord2.weight)
        torch.nn.init.xavier_uniform_(self.coord3.weight)

    def forward(self, coord):
        c = coord.view(coord.size(0), -1)
        c = torch.nn.functional.normalize(c, dim=1)
        c = self.leaky_relu(self.coord1(c))
        c = self.leaky_relu(self.coord2(c))
        c = self.coord3(c)
        c = c.view(coord.size(0), self.pref_embedding)
        return c


GAMMA_U = 0.999


class ReplayBuffer:
    def __init__(self, buffer_size_u):
        self.buffer_size_u = buffer_size_u
        self.buffer_u = deque(maxlen=buffer_size_u)

    def add_u(self, experience):
        self.buffer_u.append(experience)

    def sample_u(self, batch_size):
        return random.sample(self.buffer_u, batch_size)
        # return list(self.buffer)  # most recent, for almost-online learning

    def __len__(self):
        return len(self.buffer_u)


"""
Agent

The name is agent, but really it handles the operations with the network's
learning.

"""
class CentralizedAgent:
    def __init__(self, num_agents, action_size, buffer_size_u=10000,
                 batch_size=128):
        self.num_agents = num_agents
        # self.input_shape = input_shape
        self.action_size = action_size
        self.batch_size = batch_size

        self.u_network = UNetwork(num_agents).to(device)
        # self.u_network.load_state_dict(torch.load("model_save_simple"))
        self.u_optimizer = torch.optim.Adam(self.u_network.parameters(), lr=0.00005)
        self.memory = ReplayBuffer(buffer_size_u)

    def step(self, step_apple_reward, info_vec, new_info_vec):
        # Save experience in replay memory
        self.memory.add_u((step_apple_reward, info_vec, new_info_vec))

        if len(self.memory) >= self.batch_size:
            experiences_u = self.memory.sample_u(self.batch_size)
            self.train(experiences_u)

    # def get_actions(self, states):
        # states = torch.from_numpy(states).float().unsqueeze(0).to(device)
        # info_vec = torch.from_numpy(info_vec).float().unsqueeze(0).to(device)

    def train(self, experiences_u):
        step_apple_reward, info_vec, new_info_vec = zip(*experiences_u)
        step_apple_reward = torch.from_numpy(np.vstack(step_apple_reward)).float().to(device)
        info_vec = torch.from_numpy(np.stack(info_vec)).float().to(device)
        new_info_vec = torch.from_numpy(np.stack(new_info_vec)).float().to(device)
        # Update U function
        u_values = self.u_network(info_vec)
        next_u_values = self.u_network(new_info_vec)

        u_targets = step_apple_reward + GAMMA_U * next_u_values
        u_loss = F.mse_loss(u_values, u_targets)
        # print(f"info_vec{info_vec[5]}")
        # print(f"new_info_vec{new_info_vec[5]}")
        # print(f"U_value: {u_values[5]}, U_target:{u_targets[5]}, loss:{u_loss}")
        # print(u_loss)
        self.u_optimizer.zero_grad()
        u_loss.backward()
        self.u_optimizer.step()


thresholdDepletion = 0.4
thresholdRestoration = 0.0
wasteSpawnProbability = 0.5
appleRespawnProbability = 0.05
dirt_multiplier = 10

class CleanupEnv(MultiAgentEnv):
    """
    Cleanup environment. In this game, the agents must clean up the dirt from the river before apples can spawn.
    Agent reward is only given for eating apples, meaning the agents must learn to clean up the dirt first and
    must learn to balance their individual rewards with the collective goal of cleaning up the river.
    """

    def __init__(self, num_agents=10, height=25, width=18, greedy=False):
        """
        Initialise the environment.
        """
        self.num_agents = num_agents
        self.timestamp = 0

        self.greedy = greedy
        self.height = height
        self.width = width
        self.dirt_end = 1
        self.area = 150
        self.potential_waste_area = 150
        self.apple_start = 1

        self.agents = {}
        self.setup_simple_agent()

        self.num_dirt = 78
        self.num_apples = 0
        self.dirt_agent = self.num_agents  # init all dirt cleaner
        self.apple_agent = 0
        self.current_apple_spawn_prob = appleRespawnProbability
        self.current_waste_spawn_prob = wasteSpawnProbability
        self.compute_probabilities()

        self.total_apple_consumed = 0
        self.step_apple_consumed = 0
        self.epsilon = 0.5
        self.epsilon_decay = 0.9999

        self.heuristic = False
        self.epoch = 0

        super().__init__()

    def setup_simple_agent(self):
        for i in range(self.num_agents):
            self.agents[str(i)] = GreedyCleanUpAgent(str(i), [0, 0], -1)

    def reset(self, seed: int | None = None, options: dict = None) -> tuple:
        """
        Reset the environment.
        """
        options = options if options is not None else dict()
        # Set seed
        super().reset(seed=seed)
        self.timestamp = 0
        self.agents = {}
        self.setup_simple_agent()

        self.num_dirt = 78
        self.num_apples = 0
        self.dirt_agent = self.num_agents
        self.apple_agent = 0
        self.current_apple_spawn_prob = appleRespawnProbability
        self.current_waste_spawn_prob = wasteSpawnProbability
        self.compute_probabilities()

        self.total_apple_consumed = 0
        self.step_apple_consumed = 0

        observations = {}

        return observations, self.generate_info()

    def step(self):
        """
        Take a step in the environment.
        """
        observations = {}
        rewards = {}
        dones = {}
        self.timestamp += 1
        self.step_apple_consumed = 0
        if self.heuristic:
            agent_frequency_in_dirt = self.num_dirt / (self.num_apples + self.num_dirt)
            num_agents_to_be_assigned_to_dirt = round(self.num_agents * agent_frequency_in_dirt)
            agents_assigned_to_dirt = [agent for agent in self.agents.values() if agent.region == -1]
            agents_assigned_to_apples = [agent for agent in self.agents.values() if agent.region == 1]
            if len(agents_assigned_to_dirt) < num_agents_to_be_assigned_to_dirt:
                for i in range(num_agents_to_be_assigned_to_dirt - len(agents_assigned_to_dirt)):
                    agents_assigned_to_apples[i].region = -1
                    self.apple_agent -= 1
                    self.dirt_agent += 1
            elif len(agents_assigned_to_dirt) > num_agents_to_be_assigned_to_dirt:
                for i in range(len(agents_assigned_to_dirt) - num_agents_to_be_assigned_to_dirt):
                    agents_assigned_to_dirt[i].region = 1
                    self.dirt_agent -= 1
                    self.apple_agent += 1
        else:  # use U-network to generate roles
            d = {}
            for agent in [self.agents[key] for key in sorted(self.agents)]:
                inf = np.array([self.num_apples, self.num_dirt, self.apple_agent, self.dirt_agent])
                if (self.num_apples, self.num_dirt, self.apple_agent, self.dirt_agent) in d:
                    u_t = d[(self.num_apples, self.num_dirt, self.apple_agent, self.dirt_agent)]
                else:
                    u_input0 = torch.tensor(inf).float().unsqueeze(0).to(device)
                    u_t = centralAgent.u_network(u_input0)  # current future est.
                    d[(self.num_apples, self.num_dirt, self.apple_agent, self.dirt_agent)] = u_t

                # What if I deflect?
                if agent.region == 1:
                    self.apple_reward = u_t.item()
                    inf[2] -= 1
                    inf[3] += 1
                elif agent.region == -1:
                    self.dirt_reward = u_t.item()
                    inf[2] += 1
                    inf[3] -= 1
                new_tup = (inf[0], inf[1], inf[2], inf[3])
                if new_tup in d:
                    u_tp = d[new_tup]
                else:
                    u_input1 = torch.tensor(inf).float().unsqueeze(0).to(device)
                    u_tp = centralAgent.u_network(u_input1)
                    d[new_tup] = u_tp

                if agent.region == 1:
                    self.dirt_reward = u_tp.item()
                    self.apple_agent -= 1
                else:
                    self.apple_reward = u_tp.item()
                    self.dirt_agent -= 1

                # make decision
                if random.random() > max(self.epsilon, 0.2):
                    if self.dirt_reward >= self.apple_reward:
                        agent.region = -1
                        self.dirt_agent += 1
                    else:
                        agent.region = 1
                        self.apple_agent += 1
                else:
                    choice = np.random.choice(2)
                    if choice == 0:
                        agent.region = 1
                        self.apple_agent += 1
                    else:
                        agent.region = -1
                        self.dirt_agent += 1
        # print(f"INFO: Apple Number: {self.num_apples}, Dirt Number: {self.num_dirt}")
        # print(f"INFO: Apple Agents: {self.apple_agent}, Dirt Agents: {self.dirt_agent}")
        interim_input = np.array([self.num_apples, self.num_dirt, self.apple_agent, self.dirt_agent])

        step_reward = self.step_reward_calculation()
        self.step_apple_consumed = step_reward
        self.total_apple_consumed += step_reward
        self.step_dirt_calculation()
        # self.step_apple_consumed += self.step_dirt_calculation()*0
        # print(f"INFO: Step Reward: {step_reward}")
        self.compute_probabilities()
        new_apple, new_dirt = self.spawn_apples_and_waste()
        # print(f"INFO: New Apple: {new_apple}, New Dirt: {new_dirt}")
        # print("=======")
        self.epsilon = self.epsilon * self.epsilon_decay

        rewards["apple"] = self.total_apple_consumed
        rewards["step_apple"] = self.step_apple_consumed
        dones["__all__"] = self.timestamp == 1000
        return observations, rewards, dones, {"__all__": False}, self.generate_info(), interim_input

    def uniform_distribute(self, num_items, num_spots):
        if num_items > num_spots:
            print("Cannot distribute more items than spots.")
            return None
        indices = random.sample(range(num_spots), num_items)
        distribution = [0] * num_spots
        for index in indices:
            distribution[index] = 1
        return distribution

    def step_reward_calculation(self):
        reward = 0
        d_apple = self.uniform_distribute(self.num_apples, self.area)
        d_picker = self.uniform_distribute(self.apple_agent, self.area)
        for i in range(len(d_apple)):
            if d_apple[i] == 1 and d_picker[i] == 1:
                reward += 1
                self.num_apples -= 1
        return reward

    def step_dirt_calculation(self):
        dirt_reward = 0
        d_dirt = self.uniform_distribute(self.num_dirt, self.area)
        d_cleaner = self.uniform_distribute(self.dirt_agent, self.area)
        for i in range(len(d_dirt)):
            if d_dirt[i] == 1 and d_cleaner[i] == 1:
                self.num_dirt -= 1
                dirt_reward += 1
        return dirt_reward



    def greedily_move_to_closest_object(self):
        """
        Each agent moves to the closest object
        """
        # assert (self.greedy)
        actions = {}
        for agent in self.agents.values():
            actions[agent.agent_id] = self.get_greedy_action(agent)
        return actions

    def generate_info(self):
        return {"apple": self.num_apples, "dirt": self.num_dirt, "x1": 0,
                "x2": 0, "x3": 0, "picker": self.apple_agent, "cleaner": self.dirt_agent}

    def compute_probabilities(self):
        waste_density = 0
        if self.potential_waste_area > 0:
            waste_density = self.num_dirt / self.potential_waste_area
        if waste_density >= thresholdDepletion:
            self.current_apple_spawn_prob = 0
            self.current_waste_spawn_prob = 0
        else:
            self.current_waste_spawn_prob = wasteSpawnProbability
            if waste_density <= thresholdRestoration:
                self.current_apple_spawn_prob = appleRespawnProbability
            else:
                spawn_prob = (1 - (waste_density - thresholdRestoration)
                              / (thresholdDepletion - thresholdRestoration)) \
                             * appleRespawnProbability
                self.current_apple_spawn_prob = spawn_prob

    def spawn_apples_and_waste(self):
        # spawn apples, multiple can spawn per step
        new_apple, new_dirt = 0, 0
        for i in range(self.area):
            rand_num = np.random.rand(1)[0]
            if rand_num < self.current_apple_spawn_prob and self.num_apples < self.area:
                self.num_apples += 1
                new_apple += 1

        # spawn one waste point, only one can spawn per step
        if self.num_dirt < self.potential_waste_area:
            rand_num = np.random.rand(1)[0]
            if rand_num < self.current_waste_spawn_prob:
                self.num_dirt += 1
                new_dirt += 1
        return new_apple, new_dirt

    def find_nearest_apple_from_agent(self, agent):
        # assert (self.greedy)
        x, y = agent.pos
        closest_x, closest_y, min_distance = -1, -1, float('inf')
        for i in range(self.height):
            if self.map[i][1] == 1 and abs(i - x) + 1 <= min_distance:
                min_distance = abs(i - x) + 1
                closest_x, closest_y = i, 1
        return [closest_x, closest_y], min_distance

    def find_nearest_waste_from_agent(self, agent):
        # assert (self.greedy)
        x, y = agent.pos
        closest_x, closest_y, min_distance = -1, -1, float('inf')
        for i in range(self.height):
            if self.map[i][0] == -1 and abs(i - x) + 1 <= min_distance:
                min_distance = abs(i - x) + 1
                closest_x, closest_y = i, 1
        return [closest_x, closest_y], min_distance

    def get_greedy_action(self, agent):
        # assert (self.greedy)
        if agent.region == 1:
            nearest_obj = self.find_nearest_apple_from_agent(agent)[0]
        else:
            nearest_obj = self.find_nearest_waste_from_agent(agent)[0]
        if agent.pos[0] == nearest_obj[0]:
            if nearest_obj[1] < agent.pos[1]:
                return 3
            return 1
        if agent.pos[0] > nearest_obj[0]:
            return 0
        return 2

    def render(self):
        """
        Render the environment.
        """
        cmap = colors.ListedColormap(['tab:brown', 'white', 'green'])
        bounds = [-1, -0.5, 0.5, 1]
        norm = colors.BoundaryNorm(bounds, cmap.N)
        # create discrete colormap
        plt.rcParams["figure.figsize"] = [10, 10]
        fig, ax = plt.subplots()

        for agent in self.agents.values():
            t = "{}({}) ".format(agent.agent_id, agent.reward)
            plt.text(agent.pos[1] - 0.4, agent.pos[0], t, fontsize=8)
        ax.imshow(self.map, cmap=cmap, norm=norm)
        # draw gridlines
        ax.grid(which='major', axis='both', linestyle='-', color='k', linewidth=1)
        ax.set_xticks(np.arange(-.5, self.width, 1))
        ax.set_yticks(np.arange(-.5, self.height, 1))
        # if not labels:
        plt.tick_params(bottom=False, top=False, left=False, right=False, labelbottom=False, labelleft=False)

        plt.show()

"""Actual Training"""

env = CleanupEnv(num_agents=num_agents, greedy=True)
num_epochs = 2000
max_steps_per_epoch = 1000
input_shape = (num_agents, 4)
action_size = 2  # apple, dirt

reward_graph = []
weight_graph = defaultdict(list)
dirt_reward_graph = []

# Create environment and agent
centralAgent = CentralizedAgent(num_agents, action_size)

# Training loop
f = open("log_greedy_simple.txt", "w")
f_good = open("good_log_greedy_simple.txt", "w")
for epoch in range(num_epochs):
    print(f"=============== episode {epoch} ===============")
    f.write(f"=============== episode {epoch} ===============\n")
    # Reset environment and get initial state
    epoch_reward = 0
    max_epoch_reward = 0
    env_states, info = env.reset()
    states = preprocess_inputs(env_states)
    cur_step_apple_reward = 0
    prev_interim = np.array([info["apple"], info["dirt"], info["picker"], info["cleaner"]])
    info_vec = np.array([info["apple"], info["dirt"], info["picker"], info["cleaner"]])

    good_epoch_apple = []
    good_epoch_dirt = []
    good_epoch_x1 = []
    good_epoch_x2 = []
    good_epoch_x3 = []

    print(f"num apple: {env.num_apples}, num dirt: {env.num_dirt}")
    print(env.epsilon)

    if epoch > verbose_episode:
        verbose = True
    for step in tqdm(range(max_steps_per_epoch)):
        # Environment responds
        next_env_states, env_rewards, dones, _, info, interim_input = env.step()
        # if info["dirt"] == 0:
        #     good_epoch_apple.append(info["apple"])
        # else:
        #     good_epoch_apple.append(info["apple"]/info["dirt"])
        good_epoch_apple.append(info["apple"])
        good_epoch_dirt.append(info["dirt"])
        good_epoch_x1.append(env.dirt_agent)
        good_epoch_x2.append(info["x2"])
        good_epoch_x3.append(env.apple_agent)

        new_info_vec = np.array([info["apple"], info["dirt"], info["picker"], info["cleaner"]])
        epoch_reward = env_rewards["apple"]
        prev_step_reward = cur_step_apple_reward
        cur_step_apple_reward = env_rewards["step_apple"]

        if not env.heuristic:
            # print(f"prev_reward: {prev_step_reward}, prev_interim:{prev_interim}, cur_interim:{interim_input}")
            centralAgent.step(prev_step_reward, prev_interim, interim_input)

        # Update state
        prev_interim = interim_input
        info_vec = new_info_vec
        if verbose:
            print(f"{step}: num apple: {env.num_apples}, num dirt: {env.num_dirt}")

        if dones["__all__"]:
            break
    env.epoch += 1
    if epoch_reward >= 0:
        f_good.write(f"Epoch number: {epoch}\n")
        f_good.write(f"Epoch reward: {epoch_reward}\n")
        f_good.write(f"Epoch apple\n")
        f_good.write(f"{good_epoch_apple}\n\n")
        f_good.write(f"Epoch dirt\n")
        f_good.write(f"{good_epoch_dirt}\n\n")
        f_good.write(f"Epoch x1\n")
        f_good.write(f"{good_epoch_x1}\n\n")
        f_good.write(f"Epoch x2\n")
        f_good.write(f"{good_epoch_x2}\n\n")
        f_good.write(f"Epoch x3\n")
        f_good.write(f"{good_epoch_x3}\n\n")
    if epoch_reward > 4500 and epoch_reward >= max_epoch_reward:
        max_epoch_reward = epoch_reward
        torch.save(centralAgent.u_network.state_dict(), "model_save_simple")

    print(f"Epoch reward: {epoch_reward}")
    reward_graph.append(epoch_reward)
    print("Reward graph: ")
    print(reward_graph)
    f.write("Reward graph: \n")
    f.write(f"{reward_graph}\n")

    print(weight_graph[13])

    print(f"Ending num apple: {env.num_apples}, num dirt: {env.num_dirt}")
    print(f"Ending agents apple : {env.apple_agent}, dirt: {env.dirt_agent}")


    print(f"Epoch reward: {epoch_reward}")
    if (epoch + 1) % 100 == 0:
        print(f"Epoch {epoch} completed")
        f.write("Weight Graph: \n")

f.close()
f_good.close()
